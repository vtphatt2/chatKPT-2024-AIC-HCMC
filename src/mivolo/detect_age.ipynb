{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 268 layers, 68125494 parameters, 0 gradients, 257.4 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model meta:\n",
      "min_age: 1, max_age: 95, avg_age: 48.0, num_classes: 3, in_chans: 6, with_persons_model: True, disable_faces: False, use_persons: True, only_age: False, num_classes_gender: 2, input_size: 224, use_person_crops: True, use_face_crops: True\n",
      "Model meta:\n",
      "min_age: 1, max_age: 95, avg_age: 48.0, num_classes: 3, in_chans: 6, with_persons_model: True, disable_faces: False, use_persons: True, only_age: False, num_classes_gender: 2, input_size: 224, use_person_crops: True, use_face_crops: True\n",
      "Loaded state_dict from checkpoint 'models/mivolo_imbd.pth.tar'\n",
      "Loaded state_dict from checkpoint 'models/mivolo_imbd.pth.tar'\n",
      "Model mivolo_d1_224 created, param count: 27432414\n",
      "Model mivolo_d1_224 created, param count: 27432414\n",
      "Data processing configuration for current model + dataset:\n",
      "Data processing configuration for current model + dataset:\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinput_size: (3, 224, 224)\n",
      "\tinterpolation: bicubic\n",
      "\tinterpolation: bicubic\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tmean: (0.485, 0.456, 0.406)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tstd: (0.229, 0.224, 0.225)\n",
      "\tcrop_pct: 0.96\n",
      "\tcrop_pct: 0.96\n",
      "\tcrop_mode: center\n",
      "\tcrop_mode: center\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown input data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 154\u001b[0m\n\u001b[0;32m    150\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwriterows(data)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 81\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(args\u001b[38;5;241m.\u001b[39moutput, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     79\u001b[0m predictor \u001b[38;5;241m=\u001b[39m Predictor(args, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 81\u001b[0m input_type \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_type \u001b[38;5;241m==\u001b[39m InputType\u001b[38;5;241m.\u001b[39mVideo \u001b[38;5;129;01mor\u001b[39;00m input_type \u001b[38;5;241m==\u001b[39m InputType\u001b[38;5;241m.\u001b[39mVideoStream:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdraw:\n",
      "File \u001b[1;32md:\\ai challenge\\mivolo\\mivolo\\data\\data_reader.py:87\u001b[0m, in \u001b[0;36mget_input_type\u001b[1;34m(input_path)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m InputType\u001b[38;5;241m.\u001b[39mVideoStream\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown input data"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import yt_dlp\n",
    "from mivolo.data.data_reader import InputType, get_all_files, get_input_type\n",
    "from mivolo.predictor import Predictor\n",
    "from timm.utils import setup_default_logging\n",
    "import csv\n",
    "_logger = logging.getLogger(\"inference\")\n",
    "\n",
    "\n",
    "def get_direct_video_url(video_url):\n",
    "    ydl_opts = {\n",
    "        \"format\": \"bestvideo\",\n",
    "        \"quiet\": True,  # Suppress terminal output (remove this line if you want to see the log)\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info_dict = ydl.extract_info(video_url, download=False)\n",
    "\n",
    "        if \"url\" in info_dict:\n",
    "            direct_url = info_dict[\"url\"]\n",
    "            resolution = (info_dict[\"width\"], info_dict[\"height\"])\n",
    "            fps = info_dict[\"fps\"]\n",
    "            yid = info_dict[\"id\"]\n",
    "            return direct_url, resolution, fps, yid\n",
    "\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "def get_local_video_info(vid_uri):\n",
    "    cap = cv2.VideoCapture(vid_uri)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Failed to open video source {vid_uri}\")\n",
    "    res = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    return res, fps\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"PyTorch MiVOLO Inference\")\n",
    "    parser.add_argument(\"--input\", type=str, default=None, required=True, help=\"image file or folder with images\")\n",
    "    parser.add_argument(\"--output\", type=str, default=None, required=True, help=\"folder for output results\")\n",
    "    parser.add_argument(\"--detector-weights\", type=str, default=None, required=True, help=\"Detector weights (YOLOv8).\")\n",
    "    parser.add_argument(\"--checkpoint\", default=\"\", type=str, required=True, help=\"path to mivolo checkpoint\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--with-persons\", action=\"store_true\", default=False, help=\"If set model will run with persons, if available\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--disable-faces\", action=\"store_true\", default=False, help=\"If set model will use only persons if available\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--draw\", action=\"store_true\", default=False, help=\"If set, resulted images will be drawn\")\n",
    "    parser.add_argument(\"--device\", default=\"cuda\", type=str, help=\"Device (accelerator) to use.\")\n",
    "\n",
    "    return parser\n",
    "\n",
    "def main():\n",
    "    parser = get_parser()\n",
    "    setup_default_logging()\n",
    "    args = parser.parse_args(args=[\n",
    "        \"--input\", \"../../data\",\n",
    "        \"--output\", \"output\",\n",
    "        \"--detector-weights\", \"models/yolov8x_person_face.pt\",\n",
    "        \"--checkpoint\", \"models/mivolo_imbd.pth.tar\",\n",
    "        \"--device\", \"cuda:0\",\n",
    "        \"--with-persons\",\"--draw\"\n",
    "    ])\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    os.makedirs(args.output, exist_ok=True)\n",
    "\n",
    "    predictor = Predictor(args, verbose=True)\n",
    "\n",
    "    input_type = get_input_type(args.input)\n",
    "\n",
    "    if input_type == InputType.Video or input_type == InputType.VideoStream:\n",
    "        if not args.draw:\n",
    "            raise ValueError(\"Video processing is only supported with --draw flag. No other way to visualize results.\")\n",
    "\n",
    "        if \"youtube\" in args.input:\n",
    "            args.input, res, fps, yid = get_direct_video_url(args.input)\n",
    "            if not args.input:\n",
    "                raise ValueError(f\"Failed to get direct video url {args.input}\")\n",
    "            outfilename = os.path.join(args.output, f\"out_{yid}.avi\")\n",
    "        else:\n",
    "            bname = os.path.splitext(os.path.basename(args.input))[0]\n",
    "            outfilename = os.path.join(args.output, f\"out_{bname}.avi\")\n",
    "            res, fps = get_local_video_info(args.input)\n",
    "\n",
    "        if args.draw:\n",
    "            fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "            out = cv2.VideoWriter(outfilename, fourcc, fps, res)\n",
    "            _logger.info(f\"Saving result to {outfilename}..\")\n",
    "\n",
    "        for (detected_objects_history, frame) in predictor.recognize_video(args.input):\n",
    "            if args.draw:\n",
    "                out.write(frame)\n",
    "\n",
    "    elif input_type == InputType.Image:\n",
    "        image_files = get_all_files(args.input) if os.path.isdir(args.input) else [args.input]\n",
    "        data = []\n",
    "        for img_p in image_files:\n",
    "            img = cv2.imread(img_p)\n",
    "            detected_objects, out_im = predictor.recognize(img)\n",
    "            gender_face=[detected_objects.genders[ind] for ind in detected_objects.get_bboxes_inds('face')]\n",
    "            gender_person=[detected_objects.genders[ind] for ind in detected_objects.get_bboxes_inds('person')]\n",
    "            person = max(len(gender_person),len(gender_face))\n",
    "            \n",
    "            \n",
    "            male=0\n",
    "            ages= []\n",
    "            if len(gender_person)>len(gender_face):\n",
    "                ages = [detected_objects.ages[ind] for ind in detected_objects.get_bboxes_inds('person')]\n",
    "                for gender in gender_person:\n",
    "                 if gender == 'male' :\n",
    "                    male+=1\n",
    "            else:\n",
    "                ages = [detected_objects.ages[ind] for ind in detected_objects.get_bboxes_inds('face')]\n",
    "                for gender in gender_face:\n",
    "                 if gender == 'male' :\n",
    "                    male+=1\n",
    "            \n",
    "            name_parts = img_p.split('\\\\')[-2:]  \n",
    "            name = '\\\\'.join(name_parts)\n",
    "            \n",
    "            new_entri={\n",
    "                \"keyframe\":name,\n",
    "                \"person\":person,\n",
    "                \"male\":male,\n",
    "                \"female\":person-male,\n",
    "                \"age\":ages\n",
    "            }\n",
    "            data.append(new_entri)\n",
    "            if args.draw:\n",
    "                bname = os.path.splitext(os.path.basename(img_p))[0]\n",
    "                filename = os.path.join(args.output, f\"out_{bname}.jpg\")\n",
    "                cv2.imwrite(filename, out_im)\n",
    "                _logger.info(f\"Saved result to {filename}\")\n",
    "                \n",
    "    with open(\"output.csv\", \"w\", newline='', encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"keyframe\", \"person\", \"male\",\"female\",\"age\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
