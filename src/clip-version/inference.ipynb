{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. create folder 'clip-features-14' in each batch\n",
    "# 2. uncomment :\n",
    "\n",
    "# Phat \n",
    "filter_L = ['L01', 'L02']\n",
    "\n",
    "# # Thang Nguyen \n",
    "# filter_L = ['L03', 'L04']\n",
    "\n",
    "# # Mai Dang Khoa\n",
    "# filter_L = ['L05', 'L06']\n",
    "\n",
    "# # Khang Ly\n",
    "# filter_L = ['L07', 'L08', 'L09']\n",
    "\n",
    "# # Thang Nguyen\n",
    "# filter_L = ['L10', 'L11', 'L12']\n",
    "\n",
    "# 3. upload : https://drive.google.com/drive/folders/1bdVUUD8lsJv3_3aGBMBIzmfcqe9XaI3F?usp=share_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VoThinhPhat/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run in about 3 minutes in the first time installed\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch1': ['L01_V001',\n",
       "  'L01_V002',\n",
       "  'L01_V003',\n",
       "  'L01_V004',\n",
       "  'L01_V005',\n",
       "  'L01_V006',\n",
       "  'L01_V007',\n",
       "  'L01_V008',\n",
       "  'L01_V009',\n",
       "  'L01_V010',\n",
       "  'L01_V011',\n",
       "  'L01_V012',\n",
       "  'L01_V013',\n",
       "  'L01_V014',\n",
       "  'L01_V015',\n",
       "  'L01_V016',\n",
       "  'L01_V017',\n",
       "  'L01_V018',\n",
       "  'L01_V019',\n",
       "  'L01_V020',\n",
       "  'L01_V021',\n",
       "  'L01_V022',\n",
       "  'L01_V023',\n",
       "  'L01_V024',\n",
       "  'L01_V025',\n",
       "  'L01_V026',\n",
       "  'L01_V027',\n",
       "  'L01_V028',\n",
       "  'L01_V029',\n",
       "  'L01_V030',\n",
       "  'L01_V031',\n",
       "  'L02_V001',\n",
       "  'L02_V002',\n",
       "  'L02_V003',\n",
       "  'L02_V004',\n",
       "  'L02_V005',\n",
       "  'L02_V006',\n",
       "  'L02_V007',\n",
       "  'L02_V008',\n",
       "  'L02_V009',\n",
       "  'L02_V010',\n",
       "  'L02_V011',\n",
       "  'L02_V012',\n",
       "  'L02_V013',\n",
       "  'L02_V014',\n",
       "  'L02_V015',\n",
       "  'L02_V016',\n",
       "  'L02_V017',\n",
       "  'L02_V018',\n",
       "  'L02_V019',\n",
       "  'L02_V020',\n",
       "  'L02_V021',\n",
       "  'L02_V022',\n",
       "  'L02_V023',\n",
       "  'L02_V024',\n",
       "  'L02_V025',\n",
       "  'L02_V026',\n",
       "  'L02_V027',\n",
       "  'L02_V028',\n",
       "  'L02_V029',\n",
       "  'L02_V030',\n",
       "  'L02_V031',\n",
       "  'L03_V001',\n",
       "  'L03_V002',\n",
       "  'L03_V003',\n",
       "  'L03_V004',\n",
       "  'L03_V005',\n",
       "  'L03_V006',\n",
       "  'L03_V007',\n",
       "  'L03_V008',\n",
       "  'L03_V009',\n",
       "  'L03_V010',\n",
       "  'L03_V011',\n",
       "  'L03_V012',\n",
       "  'L03_V013',\n",
       "  'L03_V014',\n",
       "  'L03_V015',\n",
       "  'L03_V016',\n",
       "  'L03_V017',\n",
       "  'L03_V018',\n",
       "  'L03_V019',\n",
       "  'L03_V020',\n",
       "  'L03_V021',\n",
       "  'L03_V022',\n",
       "  'L03_V023',\n",
       "  'L03_V024',\n",
       "  'L03_V025',\n",
       "  'L03_V026',\n",
       "  'L03_V027',\n",
       "  'L03_V028',\n",
       "  'L03_V029',\n",
       "  'L03_V030',\n",
       "  'L04_V001',\n",
       "  'L04_V002',\n",
       "  'L04_V003',\n",
       "  'L04_V004',\n",
       "  'L04_V005',\n",
       "  'L04_V006',\n",
       "  'L04_V007',\n",
       "  'L04_V008',\n",
       "  'L04_V009',\n",
       "  'L04_V010',\n",
       "  'L04_V011',\n",
       "  'L04_V012',\n",
       "  'L04_V013',\n",
       "  'L04_V014',\n",
       "  'L04_V015',\n",
       "  'L04_V016',\n",
       "  'L04_V017',\n",
       "  'L04_V018',\n",
       "  'L04_V019',\n",
       "  'L04_V020',\n",
       "  'L04_V021',\n",
       "  'L04_V022',\n",
       "  'L04_V023',\n",
       "  'L04_V024',\n",
       "  'L04_V025',\n",
       "  'L04_V026',\n",
       "  'L04_V027',\n",
       "  'L04_V028',\n",
       "  'L04_V029',\n",
       "  'L04_V030',\n",
       "  'L05_V001',\n",
       "  'L05_V002',\n",
       "  'L05_V003',\n",
       "  'L05_V004',\n",
       "  'L05_V005',\n",
       "  'L05_V006',\n",
       "  'L05_V007',\n",
       "  'L05_V008',\n",
       "  'L05_V009',\n",
       "  'L05_V010',\n",
       "  'L05_V011',\n",
       "  'L05_V012',\n",
       "  'L05_V013',\n",
       "  'L05_V014',\n",
       "  'L05_V015',\n",
       "  'L05_V016',\n",
       "  'L05_V017',\n",
       "  'L05_V018',\n",
       "  'L05_V019',\n",
       "  'L05_V020',\n",
       "  'L05_V021',\n",
       "  'L05_V022',\n",
       "  'L05_V023',\n",
       "  'L05_V024',\n",
       "  'L05_V025',\n",
       "  'L05_V026',\n",
       "  'L05_V027',\n",
       "  'L05_V028',\n",
       "  'L05_V029',\n",
       "  'L05_V030',\n",
       "  'L05_V031',\n",
       "  'L06_V001',\n",
       "  'L06_V002',\n",
       "  'L06_V003',\n",
       "  'L06_V004',\n",
       "  'L06_V005',\n",
       "  'L06_V006',\n",
       "  'L06_V007',\n",
       "  'L06_V008',\n",
       "  'L06_V009',\n",
       "  'L06_V010',\n",
       "  'L06_V011',\n",
       "  'L06_V012',\n",
       "  'L06_V013',\n",
       "  'L06_V014',\n",
       "  'L06_V015',\n",
       "  'L06_V016',\n",
       "  'L06_V017',\n",
       "  'L06_V018',\n",
       "  'L06_V019',\n",
       "  'L06_V020',\n",
       "  'L06_V021',\n",
       "  'L06_V022',\n",
       "  'L06_V023',\n",
       "  'L06_V024',\n",
       "  'L06_V025',\n",
       "  'L06_V026',\n",
       "  'L06_V027',\n",
       "  'L06_V028',\n",
       "  'L06_V029',\n",
       "  'L06_V030',\n",
       "  'L06_V031',\n",
       "  'L07_V001',\n",
       "  'L07_V002',\n",
       "  'L07_V003',\n",
       "  'L07_V004',\n",
       "  'L07_V005',\n",
       "  'L07_V006',\n",
       "  'L07_V007',\n",
       "  'L07_V008',\n",
       "  'L07_V009',\n",
       "  'L07_V010',\n",
       "  'L07_V011',\n",
       "  'L07_V012',\n",
       "  'L07_V013',\n",
       "  'L07_V014',\n",
       "  'L07_V015',\n",
       "  'L07_V016',\n",
       "  'L07_V017',\n",
       "  'L07_V018',\n",
       "  'L07_V019',\n",
       "  'L07_V020',\n",
       "  'L07_V021',\n",
       "  'L07_V022',\n",
       "  'L07_V023',\n",
       "  'L07_V024',\n",
       "  'L07_V025',\n",
       "  'L07_V026',\n",
       "  'L07_V027',\n",
       "  'L07_V028',\n",
       "  'L07_V029',\n",
       "  'L07_V030',\n",
       "  'L07_V031',\n",
       "  'L08_V001',\n",
       "  'L08_V002',\n",
       "  'L08_V003',\n",
       "  'L08_V004',\n",
       "  'L08_V005',\n",
       "  'L08_V006',\n",
       "  'L08_V007',\n",
       "  'L08_V008',\n",
       "  'L08_V009',\n",
       "  'L08_V010',\n",
       "  'L08_V011',\n",
       "  'L08_V012',\n",
       "  'L08_V013',\n",
       "  'L08_V014',\n",
       "  'L08_V015',\n",
       "  'L08_V016',\n",
       "  'L08_V017',\n",
       "  'L08_V018',\n",
       "  'L08_V019',\n",
       "  'L08_V020',\n",
       "  'L08_V021',\n",
       "  'L08_V022',\n",
       "  'L08_V023',\n",
       "  'L08_V024',\n",
       "  'L08_V025',\n",
       "  'L08_V026',\n",
       "  'L08_V027',\n",
       "  'L08_V028',\n",
       "  'L08_V029',\n",
       "  'L08_V030',\n",
       "  'L09_V001',\n",
       "  'L09_V002',\n",
       "  'L09_V003',\n",
       "  'L09_V004',\n",
       "  'L09_V005',\n",
       "  'L09_V006',\n",
       "  'L09_V007',\n",
       "  'L09_V008',\n",
       "  'L09_V009',\n",
       "  'L09_V010',\n",
       "  'L09_V011',\n",
       "  'L09_V012',\n",
       "  'L09_V013',\n",
       "  'L09_V014',\n",
       "  'L09_V015',\n",
       "  'L09_V016',\n",
       "  'L09_V017',\n",
       "  'L09_V018',\n",
       "  'L09_V019',\n",
       "  'L09_V020',\n",
       "  'L09_V021',\n",
       "  'L09_V022',\n",
       "  'L09_V023',\n",
       "  'L09_V024',\n",
       "  'L09_V025',\n",
       "  'L09_V026',\n",
       "  'L09_V027',\n",
       "  'L09_V028',\n",
       "  'L09_V029',\n",
       "  'L10_V001',\n",
       "  'L10_V002',\n",
       "  'L10_V003',\n",
       "  'L10_V004',\n",
       "  'L10_V005',\n",
       "  'L10_V006',\n",
       "  'L10_V007',\n",
       "  'L10_V008',\n",
       "  'L10_V009',\n",
       "  'L10_V010',\n",
       "  'L10_V011',\n",
       "  'L10_V012',\n",
       "  'L10_V013',\n",
       "  'L10_V014',\n",
       "  'L10_V015',\n",
       "  'L10_V016',\n",
       "  'L10_V017',\n",
       "  'L10_V018',\n",
       "  'L10_V019',\n",
       "  'L10_V020',\n",
       "  'L10_V021',\n",
       "  'L10_V022',\n",
       "  'L10_V023',\n",
       "  'L10_V024',\n",
       "  'L10_V025',\n",
       "  'L10_V026',\n",
       "  'L10_V027',\n",
       "  'L10_V028',\n",
       "  'L10_V029',\n",
       "  'L11_V001',\n",
       "  'L11_V002',\n",
       "  'L11_V003',\n",
       "  'L11_V004',\n",
       "  'L11_V005',\n",
       "  'L11_V006',\n",
       "  'L11_V007',\n",
       "  'L11_V008',\n",
       "  'L11_V009',\n",
       "  'L11_V010',\n",
       "  'L11_V011',\n",
       "  'L11_V012',\n",
       "  'L11_V013',\n",
       "  'L11_V014',\n",
       "  'L11_V015',\n",
       "  'L11_V016',\n",
       "  'L11_V017',\n",
       "  'L11_V018',\n",
       "  'L11_V019',\n",
       "  'L11_V020',\n",
       "  'L11_V021',\n",
       "  'L11_V022',\n",
       "  'L11_V023',\n",
       "  'L11_V024',\n",
       "  'L11_V025',\n",
       "  'L11_V026',\n",
       "  'L11_V027',\n",
       "  'L11_V028',\n",
       "  'L11_V029',\n",
       "  'L11_V030',\n",
       "  'L12_V001',\n",
       "  'L12_V002',\n",
       "  'L12_V003',\n",
       "  'L12_V004',\n",
       "  'L12_V005',\n",
       "  'L12_V006',\n",
       "  'L12_V007',\n",
       "  'L12_V008',\n",
       "  'L12_V009',\n",
       "  'L12_V010',\n",
       "  'L12_V011',\n",
       "  'L12_V012',\n",
       "  'L12_V013',\n",
       "  'L12_V014',\n",
       "  'L12_V015',\n",
       "  'L12_V016',\n",
       "  'L12_V017',\n",
       "  'L12_V018',\n",
       "  'L12_V019',\n",
       "  'L12_V020',\n",
       "  'L12_V021',\n",
       "  'L12_V022',\n",
       "  'L12_V023',\n",
       "  'L12_V024',\n",
       "  'L12_V025',\n",
       "  'L12_V026',\n",
       "  'L12_V027',\n",
       "  'L12_V028',\n",
       "  'L12_V029',\n",
       "  'L12_V030']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distinct videos\n",
    "batch_videos_dict = {}\n",
    "path_pattern = os.path.join(os.getcwd(), '..', '..', 'data', \n",
    "                            'batch*', 'keyframes', '*', '*')\n",
    "paths = glob.glob(path_pattern)\n",
    "\n",
    "for path in paths:\n",
    "    _, batch_name, _, _, video_name = path.rsplit(os.sep, 4)\n",
    "    if batch_name not in batch_videos_dict:\n",
    "        batch_videos_dict[batch_name] = []\n",
    "    batch_videos_dict[batch_name].append(video_name)\n",
    "\n",
    "for batch in batch_videos_dict.keys():\n",
    "    batch_videos_dict[batch].sort()\n",
    "\n",
    "batch_videos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# be filtered at here\n",
    "for batch in batch_videos_dict.keys():\n",
    "    batch_videos_dict[batch] = [video for video in batch_videos_dict[batch] if video[:3] in filter_L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch1': ['L01_V001',\n",
       "  'L01_V002',\n",
       "  'L01_V003',\n",
       "  'L01_V004',\n",
       "  'L01_V005',\n",
       "  'L01_V006',\n",
       "  'L01_V007',\n",
       "  'L01_V008',\n",
       "  'L01_V009',\n",
       "  'L01_V010',\n",
       "  'L01_V011',\n",
       "  'L01_V012',\n",
       "  'L01_V013',\n",
       "  'L01_V014',\n",
       "  'L01_V015',\n",
       "  'L01_V016',\n",
       "  'L01_V017',\n",
       "  'L01_V018',\n",
       "  'L01_V019',\n",
       "  'L01_V020',\n",
       "  'L01_V021',\n",
       "  'L01_V022',\n",
       "  'L01_V023',\n",
       "  'L01_V024',\n",
       "  'L01_V025',\n",
       "  'L01_V026',\n",
       "  'L01_V027',\n",
       "  'L01_V028',\n",
       "  'L01_V029',\n",
       "  'L01_V030',\n",
       "  'L01_V031',\n",
       "  'L02_V001',\n",
       "  'L02_V002',\n",
       "  'L02_V003',\n",
       "  'L02_V004',\n",
       "  'L02_V005',\n",
       "  'L02_V006',\n",
       "  'L02_V007',\n",
       "  'L02_V008',\n",
       "  'L02_V009',\n",
       "  'L02_V010',\n",
       "  'L02_V011',\n",
       "  'L02_V012',\n",
       "  'L02_V013',\n",
       "  'L02_V014',\n",
       "  'L02_V015',\n",
       "  'L02_V016',\n",
       "  'L02_V017',\n",
       "  'L02_V018',\n",
       "  'L02_V019',\n",
       "  'L02_V020',\n",
       "  'L02_V021',\n",
       "  'L02_V022',\n",
       "  'L02_V023',\n",
       "  'L02_V024',\n",
       "  'L02_V025',\n",
       "  'L02_V026',\n",
       "  'L02_V027',\n",
       "  'L02_V028',\n",
       "  'L02_V029',\n",
       "  'L02_V030',\n",
       "  'L02_V031']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_videos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/001.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/002.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/003.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/004.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/005.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/006.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/007.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/008.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/009.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/010.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/011.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/012.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/013.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/014.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/015.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/016.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/017.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/018.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/019.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/020.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/021.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/022.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/023.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/024.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/025.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/026.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/027.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/028.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/029.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/030.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/031.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/032.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/033.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/034.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/035.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/036.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/037.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/038.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/039.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/040.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/041.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/042.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/043.jpg\n",
      "/Users/VoThinhPhat/Desktop/chatKPT-2024-AIC-HCMC/src/clip-version/../../data/batch1/keyframes/keyframes_L02/L02_V011/044.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(images\u001b[38;5;241m=\u001b[39mimage, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_image_features\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m image_features \u001b[38;5;241m=\u001b[39m image_features \u001b[38;5;241m/\u001b[39m image_features\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     23\u001b[0m embeddings\u001b[38;5;241m.\u001b[39mappend(image_features\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/clip/modeling_clip.py:1237\u001b[0m, in \u001b[0;36mCLIPModel.get_image_features\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1232\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1233\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1234\u001b[0m )\n\u001b[1;32m   1235\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1237\u001b[0m vision_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m vision_outputs[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# pooled_output\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_projection(pooled_output)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/clip/modeling_clip.py:1032\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m   1030\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_layrnorm(hidden_states)\n\u001b[0;32m-> 1032\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1040\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/clip/modeling_clip.py:813\u001b[0m, in \u001b[0;36mCLIPEncoder.forward\u001b[0;34m(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    805\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    806\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    807\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    810\u001b[0m         output_attentions,\n\u001b[1;32m    811\u001b[0m     )\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 813\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    820\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/clip/modeling_clip.py:548\u001b[0m, in \u001b[0;36mCLIPEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    545\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    547\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(hidden_states)\n\u001b[0;32m--> 548\u001b[0m hidden_states, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    552\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    553\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    554\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    556\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/clip/modeling_clip.py:464\u001b[0m, in \u001b[0;36mCLIPSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attention_mask\n\u001b[1;32m    462\u001b[0m bsz, tgt_len, embed_dim \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m--> 464\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[1;32m    466\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_proj(hidden_states)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for batch in batch_videos_dict.keys():\n",
    "    for video in batch_videos_dict[batch]:\n",
    "        path_save = os.path.join(os.getcwd(), '..', '..', 'data', \n",
    "                            batch, 'clip-features-14', video + \".npy\") \n",
    "        if os.path.isfile(path_save):\n",
    "            continue\n",
    "\n",
    "        path_to_keyframe = os.path.join(os.getcwd(), '..', '..', 'data', \n",
    "                            batch, 'keyframes', 'keyframes_' + video[:3], video) \n",
    "        print(path_to_keyframe)\n",
    "\n",
    "        embeddings = []\n",
    "        image_files = [os.path.join(path_to_keyframe, f) for f in os.listdir(path_to_keyframe) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        image_files.sort()\n",
    "\n",
    "        for image_path in image_files:\n",
    "            print(image_path)\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            inputs = processor(images=image, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**inputs)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            embeddings.append(image_features.cpu().numpy().flatten())\n",
    "        embeddings_array = np.array(embeddings)\n",
    "        path_save = os.path.join(os.getcwd(), '..', '..', 'data', \n",
    "                            batch, 'clip-features-14', video + \".npy\") \n",
    "        np.save(path_save, embeddings_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
