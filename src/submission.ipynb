{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████| 106589/106589 [8.9s elapsed, 0s remaining, 11.8K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# run in about 15 seconds\n",
    "if fo.dataset_exists(\"AIC_2024\"):\n",
    "    fo.delete_dataset(\"AIC_2024\")\n",
    "    \n",
    "dataset = fo.Dataset.from_images_dir(\n",
    "    name=\"AIC_2024\", \n",
    "    images_dir=os.path.join(\"..\", \"data\"), \n",
    "    recursive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in about 36 seconds\n",
    "unique_videos = set()\n",
    "for sample in dataset:\n",
    "    tmp, sample['video'], sample['keyframe_id'] = sample['filepath'][:-4].rsplit(os.sep, 2)\n",
    "    sample['batch'] = tmp.rsplit(os.sep, 4)[-3]\n",
    "    unique_videos.add(sample['video'])\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in nearly 40 seconds\n",
    "video_frameid_dict = {}\n",
    "for b in [1, 2, 3]:\n",
    "    for video in unique_videos:\n",
    "        filepath = os.path.join('..', 'data', f'batch{b}', 'map-keyframes', f'{video}.csv')\n",
    "        if os.path.exists(filepath):\n",
    "            a = pd.read_csv(filepath)\n",
    "            video_frameid_dict[video] = a['frame_idx']\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['frame_id'] = video_frameid_dict[sample['video']].iloc[int(sample['keyframe_id']) - 1]\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in about 1 minutes\n",
    "video_keyframe_dict = {}\n",
    "all_keyframe_paths = glob(os.path.join(os.getcwd(), '..', 'data', 'batch*', 'keyframes',\n",
    "                            '*', '*', '*.jpg'))\n",
    "\n",
    "for kf in all_keyframe_paths:\n",
    "    _, vid, kf = kf[:-4].rsplit(os.sep, 2)\n",
    "    if vid not in video_keyframe_dict.keys():\n",
    "        video_keyframe_dict[vid] = [kf]\n",
    "    else:\n",
    "        video_keyframe_dict[vid].append(kf)\n",
    "\n",
    "for k, v in video_keyframe_dict.items():\n",
    "    video_keyframe_dict[k] = sorted(v)\n",
    "\n",
    "embedding_dict = {}\n",
    "for j in [1, 2, 3]:\n",
    "    for video in unique_videos:\n",
    "        clip_14_path = os.path.join('..', 'data', f'batch{j}', \n",
    "                            'clip-features-14', f'{video}.npy')\n",
    "        if os.path.exists(clip_14_path):\n",
    "            a = np.load(clip_14_path)\n",
    "            embedding_dict[video] = {}\n",
    "            for i, k in enumerate(video_keyframe_dict[video]):\n",
    "                embedding_dict[video][k] = a[i]\n",
    "\n",
    "for sample in dataset:\n",
    "    sample['clip-14'] = embedding_dict[sample['video']][sample['keyframe_id']]\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/VoThinhPhat/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# run in 10 minutes\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in 11 seconds\n",
    "image_samples = []\n",
    "image_embeddings = []\n",
    "for sample in dataset:\n",
    "    image_samples.append(sample)\n",
    "    image_embeddings.append(sample['clip-14']) \n",
    "image_embeddings = np.array(image_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission(text_query, k, csv_file):\n",
    "    inputs = processor(text=[text_query], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs).cpu().numpy().flatten()\n",
    "    similarities = cosine_similarity([text_features], image_embeddings)[0]\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "\n",
    "    if fo.dataset_exists(\"submission\"):\n",
    "        fo.delete_dataset(\"submission\")\n",
    "\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"submission\"\n",
    "    )\n",
    "\n",
    "    for index in top_k_indices:\n",
    "        dataset_submission.add_sample(image_samples[index])\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        writer.writeheader()\n",
    "        for sample in dataset_submission:\n",
    "            writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "text_query = \"a rescue boat attempting to rescue a drowning man. The boat uses an orange buoy to pull the man closer to the boat. The scene then cuts to a man in a white shirt being interviewed\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission(text_query, 100, output_file)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
