{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors \n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "from routes import init_routes\n",
    "sys.path.append(\"task-former/code\")\n",
    "from clip.model import convert_weights, CLIP\n",
    "from clip.clip import _transform, load, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in about 25 seconds\n",
    "if fo.dataset_exists(\"AIC_2024\"):\n",
    "    fo.delete_dataset(\"AIC_2024\")\n",
    "    \n",
    "dataset = fo.Dataset.from_images_dir(\n",
    "    name=\"AIC_2024\", \n",
    "    images_dir=os.path.join(\"..\", \"data\"), \n",
    "    recursive=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in about 1 minutes 15 seconds\n",
    "unique_videos = set()\n",
    "for sample in dataset:\n",
    "    tmp, sample['video'], sample['keyframe_id'] = sample['filepath'][:-4].rsplit(os.sep, 2)\n",
    "    sample['batch'] = tmp.rsplit(os.sep, 4)[-3]\n",
    "    unique_videos.add(sample['video'])\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_samples = []\n",
    "image_clip14_embeddings = []\n",
    "image_task_former_embedding = []\n",
    "submission_samples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in about 4 minutes\n",
    "video_keyframe_dict = {}\n",
    "all_keyframe_paths = glob(os.path.join(os.getcwd(), '..', 'data', 'batch*', 'keyframes',\n",
    "                            '*', '*', '*.jpg'))\n",
    "\n",
    "video_frameid_dict = {}\n",
    "for b in [1, 2, 3]:\n",
    "    for video in unique_videos:\n",
    "        filepath = os.path.join('..', 'data', f'batch{b}', 'map-keyframes', f'{video}.csv')\n",
    "        if os.path.exists(filepath):\n",
    "            a = pd.read_csv(filepath)\n",
    "            video_frameid_dict[video] = a['frame_idx']\n",
    "\n",
    "for kf in all_keyframe_paths:\n",
    "    _, vid, kf = kf[:-4].rsplit(os.sep, 2)\n",
    "    if vid not in video_keyframe_dict.keys():\n",
    "        video_keyframe_dict[vid] = [kf]\n",
    "    else:\n",
    "        video_keyframe_dict[vid].append(kf)\n",
    "\n",
    "for k, v in video_keyframe_dict.items():\n",
    "    video_keyframe_dict[k] = sorted(v)\n",
    "\n",
    "embedding_clip14_dict = {}\n",
    "embedding_task_former_dict = {}\n",
    "for j in [1, 2, 3]:\n",
    "    for video in unique_videos:\n",
    "        clip14_path = os.path.join('..', 'data', f'batch{j}', \n",
    "                            'clip-features-14', f'{video}.npy')\n",
    "        if os.path.exists(clip14_path):\n",
    "            a = np.load(clip14_path)\n",
    "            embedding_clip14_dict[video] = {}\n",
    "            for i, k in enumerate(video_keyframe_dict[video]):\n",
    "                embedding_clip14_dict[video][k] = a[i]\n",
    "\n",
    "        task_former_path = os.path.join('..', 'data', f'batch{j}', \n",
    "                    'task-former', f'{video}.npy')\n",
    "        if os.path.exists(task_former_path):\n",
    "            b = np.load(task_former_path)\n",
    "            embedding_task_former_dict[video] = {}\n",
    "            for i, k in enumerate(video_keyframe_dict[video]):\n",
    "                embedding_task_former_dict[video][k] = b[i]\n",
    "\n",
    "for sample in dataset:\n",
    "    print(sample['video'] + ' - ' + sample['keyframe_id'], end='')\n",
    "    \n",
    "    sample['frame_id'] = video_frameid_dict[sample['video']].iloc[int(sample['keyframe_id']) - 1]\n",
    "    sample['clip-14'] = embedding_clip14_dict[sample['video']][sample['keyframe_id']]\n",
    "    sample['task-former'] = embedding_task_former_dict[sample['video']][sample['keyframe_id']]\n",
    "    image_samples.append(sample)\n",
    "    image_clip14_embeddings.append(sample['clip-14']) \n",
    "    image_task_former_embedding.append(sample['task-former'])\n",
    "\n",
    "    print(\" ---  Done\")\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_clip14_embeddings = np.array(image_clip14_embeddings)\n",
    "image_task_former_embeddings = np.array(image_task_former_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in 20 seconds\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"  # Use GPU with CUDA\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Use Metal Performance Shaders for Apple Silicon\n",
    "else:\n",
    "    device = \"cpu\"  # Default to CPU\n",
    "\n",
    "print(f\"Using: {device}\")\n",
    "\n",
    "model_clip14 = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14-336\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14-336\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_clip14(text_query, k, csv_file, bias, discard_videos):\n",
    "    inputs = processor(text=[text_query], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model_clip14.get_text_features(**inputs).cpu().numpy().flatten()\n",
    "    similarities = cosine_similarity([text_features], image_clip14_embeddings)[0]\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "\n",
    "    if fo.dataset_exists(\"submission_clip14\"):\n",
    "        fo.delete_dataset(\"submission_clip14\")\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"submission_clip14\"\n",
    "    )\n",
    "\n",
    "    count = 1\n",
    "    visited = [False] * k\n",
    "    for index in range(0, k):\n",
    "        if (not visited[index]):\n",
    "            x = []\n",
    "            x.append(image_samples[top_k_indices[index]])\n",
    "            visited[index] = True\n",
    "            for j in range(index + 1, k):\n",
    "                if (image_samples[top_k_indices[index]]['video'] == image_samples[top_k_indices[j]]['video']\n",
    "                    and abs(image_samples[top_k_indices[index]]['frame_id'] - image_samples[top_k_indices[j]]['frame_id']) < bias):\n",
    "                    visited[j] = True\n",
    "                    x.append(image_samples[top_k_indices[j]])\n",
    "                    continue\n",
    "            x = sorted(x, key=lambda a:int(a['frame_id']))\n",
    "            for e in x:\n",
    "                e['cluster'] = f'cluster {count}'\n",
    "                dataset_submission.add_sample(e)\n",
    "            count += 1\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()\n",
    "        for sample in dataset_submission:\n",
    "            if (sample['video'] not in discard_videos):\n",
    "                writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_clip14_combined(text_query1, weight1, text_query2, weight2, k, csv_file, bias):\n",
    "    total_weight = weight1 + weight2\n",
    "    if total_weight == 0:\n",
    "        raise ValueError(\"The sum of weight1 and weight2 must not be zero.\")\n",
    "    normalized_weight1 = weight1 / total_weight\n",
    "    normalized_weight2 = weight2 / total_weight\n",
    "\n",
    "    inputs1 = processor(text=[text_query1], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features1 = model_clip14.get_text_features(**inputs1).cpu().numpy().flatten()\n",
    "\n",
    "    inputs2 = processor(text=[text_query2], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features2 = model_clip14.get_text_features(**inputs2).cpu().numpy().flatten()\n",
    "\n",
    "    combined_text_features = (normalized_weight1 * text_features1) + (normalized_weight2 * text_features2)\n",
    "\n",
    "    similarities = cosine_similarity([combined_text_features], image_clip14_embeddings)[0]\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "\n",
    "    if fo.dataset_exists(\"submission_clip14_combined\"):\n",
    "        fo.delete_dataset(\"submission_clip14_combined\")\n",
    "    dataset_submission = fo.Dataset(name=\"submission_clip14_combined\")\n",
    "\n",
    "    count = 1\n",
    "    visited = [False] * k\n",
    "    for index in range(k):\n",
    "        if not visited[index]:\n",
    "            cluster_samples = []\n",
    "            current_index = top_k_indices[index]\n",
    "            cluster_samples.append(image_samples[current_index])\n",
    "            visited[index] = True\n",
    "\n",
    "            for j in range(index + 1, k):\n",
    "                if not visited[j]:\n",
    "                    compare_index = top_k_indices[j]\n",
    "                    if (image_samples[current_index]['video'] == image_samples[compare_index]['video'] and\n",
    "                        abs(image_samples[current_index]['frame_id'] - image_samples[compare_index]['frame_id']) < bias):\n",
    "                        cluster_samples.append(image_samples[compare_index])\n",
    "                        visited[j] = True\n",
    "\n",
    "            cluster_samples = sorted(cluster_samples, key=lambda x: int(x['frame_id']))\n",
    "            for sample in cluster_samples:\n",
    "                sample['cluster'] = f'cluster {count}'\n",
    "                dataset_submission.add_sample(sample)\n",
    "            count += 1\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()  # Uncommented to include headers\n",
    "        for sample in dataset_submission:\n",
    "            writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_clip14_intersection(text_query1, text_query2, k, csv_file, bias):\n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"Parameter 'k' must be a positive integer.\")\n",
    "\n",
    "    inputs1 = processor(text=[text_query1], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features1 = model_clip14.get_text_features(**inputs1).cpu().numpy().flatten()\n",
    "\n",
    "    similarities1 = cosine_similarity([text_features1], image_clip14_embeddings)[0]\n",
    "    top_k_indices1 = similarities1.argsort()[-k:][::-1]\n",
    "    top_k_set1 = set(top_k_indices1)\n",
    "\n",
    "    inputs2 = processor(text=[text_query2], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features2 = model_clip14.get_text_features(**inputs2).cpu().numpy().flatten()\n",
    "\n",
    "    similarities2 = cosine_similarity([text_features2], image_clip14_embeddings)[0]\n",
    "    top_k_indices2 = similarities2.argsort()[-k:][::-1]\n",
    "    top_k_set2 = set(top_k_indices2)\n",
    "\n",
    "    intersection_indices = list(top_k_set1.intersection(top_k_set2))\n",
    "\n",
    "    if not intersection_indices:\n",
    "        print(\"No common images found between the two top-k sets.\")\n",
    "        return fo.Dataset()\n",
    "\n",
    "    if fo.dataset_exists(\"submission_clip14_intersection\"):\n",
    "        fo.delete_dataset(\"submission_clip14_intersection\")\n",
    "    dataset_submission = fo.Dataset(name=\"submission_clip14_intersection\")\n",
    "\n",
    "    count = 1\n",
    "    visited = [False] * len(intersection_indices)\n",
    "    for i in range(len(intersection_indices)):\n",
    "        if not visited[i]:\n",
    "            cluster_samples = []\n",
    "            current_index = intersection_indices[i]\n",
    "            cluster_samples.append(image_samples[current_index])\n",
    "            visited[i] = True\n",
    "\n",
    "            for j in range(i + 1, len(intersection_indices)):\n",
    "                if not visited[j]:\n",
    "                    compare_index = intersection_indices[j]\n",
    "                    if (image_samples[current_index]['video'] == image_samples[compare_index]['video'] and\n",
    "                        abs(int(image_samples[current_index]['frame_id']) - int(image_samples[compare_index]['frame_id'])) < bias):\n",
    "                        cluster_samples.append(image_samples[compare_index])\n",
    "                        visited[j] = True\n",
    "\n",
    "            cluster_samples = sorted(cluster_samples, key=lambda x: int(x['frame_id']))\n",
    "\n",
    "            for sample in cluster_samples:\n",
    "                sample['cluster'] = f'cluster {count}'\n",
    "                dataset_submission.add_sample(sample)\n",
    "            count += 1\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()  # Include headers in the CSV\n",
    "        for sample in dataset_submission:\n",
    "            writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in 11 seconds\n",
    "model_config_file = os.path.join(os.getcwd(), 'task-former', 'code', 'training', \n",
    "                                 'model_configs', 'ViT-B-16.json')\n",
    "# remember to download file weights .pt from github (or you can contact Thinh Phat)\n",
    "model_file = os.path.join(os.getcwd(), 'task-former', 'model', 'tsbir_model_final.pt')\n",
    "\n",
    "with open(model_config_file, 'r') as f:\n",
    "    model_info = json.load(f)\n",
    "\n",
    "model_task_former = CLIP(**model_info)\n",
    "loc = device\n",
    "checkpoint = torch.load(model_file, map_location=loc)\n",
    "\n",
    "sd = checkpoint[\"state_dict\"]\n",
    "if next(iter(sd.items()))[0].startswith('module'):\n",
    "    sd = {k[len('module.'):]: v for k, v in sd.items()}\n",
    "\n",
    "model_task_former.load_state_dict(sd, strict=False)\n",
    "model_task_former.eval()\n",
    "model_task_former = model_task_former.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_weights(model_task_former)\n",
    "preprocess_val = _transform(model_task_former.visual.input_resolution, is_train=False)\n",
    "transformer = preprocess_val\n",
    "def get_feature(query_sketch, query_text):\n",
    "    img1 = transformer(query_sketch).unsqueeze(0).to(device)\n",
    "    txt = tokenize([str(query_text)])[0].unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sketch_feature = model_task_former.encode_sketch(img1)\n",
    "        text_feature = model_task_former.encode_text(txt)\n",
    "        text_feature = text_feature / text_feature.norm(dim=-1, keepdim=True)\n",
    "        sketch_feature = sketch_feature / sketch_feature.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    return model_task_former.feature_fuse(sketch_feature,text_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbrs = NearestNeighbors(algorithm='brute', metric='cosine').fit(image_task_former_embedding)\n",
    "\n",
    "def submission_task_former(text_query, sketch_path, k, csv_file, bias):\n",
    "    sketch = Image.open(sketch_path)\n",
    "    query_feat = get_feature(sketch, text_query).cpu().numpy()\n",
    "\n",
    "    nbrs.n_neighbors = k\n",
    "    distances, indices = nbrs.kneighbors(query_feat)\n",
    "\n",
    "    if fo.dataset_exists(\"submission_task_former\"):\n",
    "        fo.delete_dataset(\"submission_task_former\")\n",
    "\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"submission_task_former\"\n",
    "    )\n",
    "\n",
    "    count = 1\n",
    "    visited = [False] * k\n",
    "    for index in range(0, k):\n",
    "        if (not visited[index]):\n",
    "            x = []\n",
    "            x.append(image_samples[indices[0][index]])\n",
    "            visited[index] = True\n",
    "            for j in range(index + 1, k):\n",
    "                if (not visited[j]):\n",
    "                    if (image_samples[indices[0][index]]['video'] == image_samples[indices[0][j]]['video']\n",
    "                        and abs(image_samples[indices[0][index]]['frame_id'] - image_samples[indices[0][j]]['frame_id']) < bias):\n",
    "                        visited[j] = True\n",
    "                        x.append(image_samples[indices[0][j]])\n",
    "                        continue\n",
    "            x = sorted(x, key=lambda a:int(a['frame_id']))\n",
    "            for e in x:\n",
    "                e['cluster'] = f'cluster {count}'\n",
    "                dataset_submission.add_sample(e)\n",
    "            count += 1\n",
    "\n",
    "    for index in indices[0]:\n",
    "        dataset_submission.add_sample(image_samples[index])\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()\n",
    "        for sample in dataset_submission:\n",
    "            writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = NearestNeighbors(algorithm='brute', metric='cosine')\n",
    "\n",
    "def fillResult(csv_file):\n",
    "    csv_file = os.path.join('..', 'submission', csv_file)\n",
    "    \n",
    "    idx_result = []\n",
    "    final_result = []\n",
    "    clip14_result = []\n",
    "\n",
    "    if fo.dataset_exists(\"submission_fillResult\"):\n",
    "        fo.delete_dataset(\"submission_fillResult\")\n",
    "\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"submission_fillResult\"\n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(csv_file, header=None, names=['video', 'frame_id'])\n",
    "    for index, row in df.iterrows():\n",
    "        for i in range(len(image_samples)):\n",
    "            if (image_samples[i]['video'] == row['video']\n",
    "                and image_samples[i]['frame_id'] == row['frame_id']):\n",
    "                final_result.append(image_samples[i])\n",
    "                clip14_result.append(image_samples[i]['clip-14'])\n",
    "                idx_result.append(i)\n",
    "                continue\n",
    "\n",
    "    nn_model.n_neighbors = 100\n",
    "    nn_model.fit(image_clip14_embeddings)\n",
    "    clip14_result_array = np.array(clip14_result)\n",
    "    distances, indices = nn_model.kneighbors(clip14_result_array)\n",
    "\n",
    "    for idx in indices[0]:\n",
    "        if (idx not in idx_result):\n",
    "            final_result.append(image_samples[idx])\n",
    "            dataset_submission.add_sample(image_samples[idx])\n",
    "\n",
    "    with open(csv_file, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()\n",
    "        for i in range(0, 100):\n",
    "            writer.writerow({'video': final_result[i]['video'], 'frame_id': final_result[i]['frame_id']})\n",
    "\n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillRange(csv_file, video_name, low_frame_id, high_frame_id, mode):\n",
    "    csv_file = os.path.join('..', 'submission', csv_file)\n",
    "    with open(csv_file, mode=mode, newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "        # writer.writeheader()\n",
    "        for i in range(low_frame_id, high_frame_id + 1):\n",
    "            writer.writerow({'video': video_name, 'frame_id': i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "\n",
    "def help(indices, i, dataset_submission, bias):\n",
    "    if (i == len(indices)):\n",
    "        for num in path:\n",
    "            dataset_submission.add_sample(image_samples[num])\n",
    "        return\n",
    "    \n",
    "    for idx in indices[i]:\n",
    "        if (not path):\n",
    "            path.append(idx)\n",
    "            help(indices, i + 1, dataset_submission, bias)\n",
    "            path.pop()\n",
    "        else:\n",
    "            if (image_samples[idx]['video'] == image_samples[path[-1]]['video']\n",
    "                 and image_samples[idx]['frame_id'] > image_samples[path[-1]]['frame_id']\n",
    "                 and image_samples[idx]['frame_id'] - image_samples[path[-1]]['frame_id'] < bias):\n",
    "                path.append(idx)\n",
    "                help(indices, i + 1, dataset_submission, bias)\n",
    "                path.pop()\n",
    "\n",
    "def search_by_continuous_scene(text_query, k, bias):\n",
    "    scenes = text_query.split('/ ')\n",
    "    indices = []\n",
    "    path.clear()\n",
    "    for text in scenes:\n",
    "        inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_features1 = model_clip14.get_text_features(**inputs).cpu().numpy().flatten()\n",
    "        similarities = cosine_similarity([text_features1], image_clip14_embeddings)[0]\n",
    "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "        indices.append(top_k_indices)\n",
    "\n",
    "    if fo.dataset_exists(\"search_by_continuous_scene\"):\n",
    "        fo.delete_dataset(\"search_by_continuous_scene\")\n",
    "\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"search_by_continuous_scene\"\n",
    "    )\n",
    "\n",
    "    help(indices, 0, dataset_submission, bias)\n",
    "    \n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seePrePost(video_name, frame_id):\n",
    "    if fo.dataset_exists(\"pre_and_post\"):\n",
    "        fo.delete_dataset(\"pre_and_post\")\n",
    "\n",
    "    dataset_submission = fo.Dataset(\n",
    "        name=\"pre_and_post\"\n",
    "    )\n",
    "\n",
    "    closest_index = -1\n",
    "    diff = 30000\n",
    "\n",
    "    closest_index = -1\n",
    "    min_diff = float('inf')\n",
    "\n",
    "    # Tìm chỉ số với frame_id gần nhất\n",
    "    for index in range(len(image_samples)):\n",
    "        if image_samples[index]['video'] == video_name:\n",
    "            current_frame_id = image_samples[index]['frame_id']\n",
    "            diff = abs(current_frame_id - frame_id)\n",
    "            if diff < min_diff:\n",
    "                min_diff = diff\n",
    "                closest_index = index\n",
    "\n",
    "    # Nếu tìm thấy frame_id gần nhất, thêm các mẫu xung quanh nó\n",
    "    if closest_index != -1:\n",
    "        low = max(0, closest_index - 10)\n",
    "        high = min(len(image_samples), closest_index + 10)\n",
    "        for i in range(low, high):\n",
    "            dataset_submission.add_sample(image_samples[i])\n",
    "            \n",
    "    return dataset_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = \"The trailer is about a food festival. However, there is a scene in the trailer where you use virtual reality devices. There is a scene in the trailer where an interviewee is wearing a pink shirt, with a pink backdrop in the background.\"\n",
    "output_file = \"output.csv\"\n",
    "discard_videos = ['L17_V003']\n",
    "\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission_clip14(text_query, 200, output_file, 500, discard_videos)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_query = 'many people in a stage'\n",
    "sketch_path = \"task-former/my_drawing.png\"\n",
    "output_file = \"output.csv\"\n",
    "\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission_task_former(text_query, sketch_path, k=100, csv_file=output_file, bias=500)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"The news story tells of the lives of people affected by natural disasters. The news story begins with scenes of many houses destroyed.\"\n",
    "query2 = \"The news story shows a man in a dark coat and hat visiting a woman in a pink shirt. The news story ends with a rescuer with a blue flashlight on his head trying to save a person buried in the ground.\"\n",
    "weight1 = 0.5\n",
    "weight2 = 0.5\n",
    "\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission_clip14_combined(query1, weight1, query2, weight2, 100, \n",
    "                                                output_file, 500)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"many people wearing red scarves\"\n",
    "scene_categories = ['stage', 'performance']\n",
    "weight1 = 0.5\n",
    "weight2 = 0.5\n",
    "\n",
    "query2 = f\"A photo of {', '.join(scene_categories)}\"\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission_clip14_combined(query1, weight1, query2, weight2, 100, \n",
    "                                                output_file, 500)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"a stage\"\n",
    "query2 = \"there are many students wearing red scarves.\"\n",
    "\n",
    "output_file = os.path.join('..', 'submission', output_file)\n",
    "dataset_submission = submission_clip14_intersection(query1, query2, 1000, output_file, 500)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_submission = fillResult(\"output.csv\")\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillRange(\"query-p1-22-kis.csv\", \"L12_V013\", 7375, 7444, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_submission = seePrePost(\"L10_V016\", 6355)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "text_query = 'The police standing around a person wearing a light brown shirt (with a tattoo on his arm)./ There are 2 people wearing black shirts. The others are wearing yellow reflective vests./ Someone is holding a phone to record./ a police officer working.'\n",
    "k = 2000\n",
    "bias = 250\n",
    "dataset_submission = search_by_continuous_scene(text_query, k, bias)\n",
    "session = fo.launch_app(dataset_submission, auto=False)\n",
    "# session.open_tab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# def sort_by_middle_number(file_list):\n",
    "#     def extract_middle_number(filename):\n",
    "#         match = re.search(r'query-p1-(\\d+)-', filename)\n",
    "#         if match:\n",
    "#             return int(match.group(1)) \n",
    "#         return 0  \n",
    "    \n",
    "#     sorted_list = sorted(file_list, key=extract_middle_number)\n",
    "#     return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMISSION_FOLDER = os.path.join('..', 'submission')\n",
    "\n",
    "# app = Flask(__name__)\n",
    "# init_routes(app)\n",
    "\n",
    "# # Route to handle form submission\n",
    "# @app.route('/submit_text', methods=['POST'])\n",
    "# def submit_text():\n",
    "#     user_text = request.form['inputText']\n",
    "#     selected_csv_file = request.form.get('file_name')\n",
    "#     output_file = request.form.get('output_file')\n",
    "#     k = int(request.form.get('no_images'))\n",
    "#     bias = int(request.form.get('bias'))\n",
    "#     notify_interact_text = ''\n",
    "\n",
    "#     print(bias)\n",
    "\n",
    "#     csv_files = [f for f in os.listdir(SUBMISSION_FOLDER) if f.endswith('.csv')]\n",
    "#     csv_files = sort_by_middle_number(csv_files)\n",
    "#     notify_interact_text = f\"Open {selected_csv_file} successfully !\"\n",
    "\n",
    "#     print(user_text)\n",
    "\n",
    "#     dataset_submission = submission_clip14(user_text, k, os.path.join('..', 'submission', output_file), bias)\n",
    "#     submission_samples.clear()\n",
    "#     for sample in dataset_submission:\n",
    "#         submission_samples.append(sample)\n",
    "#     session = fo.launch_app(dataset_submission, auto=False)\n",
    "\n",
    "#     with open(os.path.join('..', 'submission', output_file), newline='', encoding='utf-8') as csvfile:\n",
    "#         reader = csv.reader(csvfile)\n",
    "#         csv_content = \"\\n\".join([\", \".join(row) for row in reader]) \n",
    "\n",
    "#     processed_text = f'Available at <a href=\"http://localhost:5151/datasets/submission_clip14\">http://localhost:5151/datasets/submission_clip14</a>'\n",
    "\n",
    "#     return render_template('index.html', notify_submit_text=user_text, processed_text=processed_text,\n",
    "#                            selected_csv_file=selected_csv_file, csv_files=csv_files,\n",
    "#                            csv_content=csv_content, notify_interact_text=notify_interact_text,\n",
    "#                            user_text = user_text, output_file=output_file, no_images=k, bias=bias)\n",
    "\n",
    "# @app.route('/write_selected_imgs_to_file', methods=['POST'])\n",
    "# def write_selected_imgs_to_file():\n",
    "#     user_text = request.form['inputText']\n",
    "#     selected_csv_file = request.form.get('file_name')\n",
    "#     output_file = request.form.get('output_file')\n",
    "#     file_name = request.form.get('file_name')\n",
    "#     bias = int(request.form.get('bias'))\n",
    "#     k = int(request.form.get('no_images'))\n",
    "#     csv_file = os.path.join('..', 'submission', file_name)\n",
    "#     csv_files = [f for f in os.listdir(SUBMISSION_FOLDER) if f.endswith('.csv')]\n",
    "#     csv_files = sort_by_middle_number(csv_files)\n",
    "\n",
    "#     if (csv_file.endswith(\".csv\")):\n",
    "#         with open(csv_file, mode='w', newline='') as file:\n",
    "#             writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "#             # writer.writeheader()\n",
    "#             for id in session.selected:\n",
    "#                 print(id)\n",
    "#                 for sample in submission_samples:\n",
    "#                     if (sample['id'] == id):\n",
    "#                         writer.writerow({'video': sample['video'], 'frame_id': sample['frame_id']})\n",
    "#                         break\n",
    "#         with open(csv_file, newline='', encoding='utf-8') as csvfile:\n",
    "#             reader = csv.reader(csvfile)\n",
    "#             csv_content = \"\\n\".join([\", \".join(row) for row in reader]) \n",
    "\n",
    "#     notify_interact_text = f\"Write to {file_name} successfully\" \n",
    "#     return render_template('index.html', notify_submit_text=user_text,\n",
    "#                            selected_csv_file=selected_csv_file, csv_files=csv_files,\n",
    "#                            csv_content=csv_content, notify_interact_text=notify_interact_text,\n",
    "#                            user_text = user_text, output_file=output_file, no_images=k, bias=bias)\n",
    "\n",
    "# @app.route('/discard_video', methods=['POST'])\n",
    "# def discard_video():\n",
    "#     file_name = request.form.get('file_name')\n",
    "#     discard_video = request.form.get('discard_videos')\n",
    "#     user_text = request.form.get('inputText')\n",
    "#     selected_csv_file = request.form.get('file_name')\n",
    "#     output_file = request.form.get('output_file')\n",
    "#     bias = int(request.form.get('bias'))\n",
    "#     k = int(request.form.get('no_images'))\n",
    "#     csv_files = [f for f in os.listdir(SUBMISSION_FOLDER) if f.endswith('.csv')]\n",
    "#     csv_files = sort_by_middle_number(csv_files)\n",
    "\n",
    "#     if (file_name and discard_video):\n",
    "#         file_path = os.path.join(SUBMISSION_FOLDER, file_name)\n",
    "#         discard_videos = discard_video.split('\\r\\n')\n",
    "#         a = pd.read_csv(file_path, header=None, names=['video', 'frame_id'])\n",
    "#         with open(file_path, mode='w', newline='') as file:\n",
    "#             writer = csv.DictWriter(file, fieldnames=['video', 'frame_id'])\n",
    "#             # writer.writeheader()\n",
    "#             for i, row in a.iterrows():\n",
    "#                 if (row['video'] not in discard_videos):\n",
    "#                     writer.writerow({'video': row['video'], 'frame_id': row['frame_id']})\n",
    "\n",
    "#         with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
    "#             reader = csv.reader(csvfile)\n",
    "#             csv_content = \"\\n\".join([\", \".join(row) for row in reader]) \n",
    "\n",
    "#         notify_interact_text = f\"Discard successfully\" \n",
    "\n",
    "#     return render_template('index.html', notify_submit_text=user_text,\n",
    "#                     selected_csv_file=selected_csv_file, csv_files=csv_files,\n",
    "#                     csv_content=csv_content, notify_interact_text=notify_interact_text,\n",
    "#                     user_text = user_text, output_file=output_file, no_images=k, bias=bias,\n",
    "#                     discard_video=discard_video)\n",
    "\n",
    "# @app.route('/see_pre_and_post', methods=['POST'])\n",
    "# def see_pre_and_post():\n",
    "#     file_name = request.form.get('file_name')\n",
    "#     discard_video = request.form.get('discard_videos')\n",
    "#     user_text = request.form.get('inputText')\n",
    "#     selected_csv_file = request.form.get('file_name')\n",
    "#     output_file = request.form.get('output_file')\n",
    "#     bias = int(request.form.get('bias'))\n",
    "#     k = int(request.form.get('no_images'))\n",
    "#     csv_files = [f for f in os.listdir(SUBMISSION_FOLDER) if f.endswith('.csv')]\n",
    "#     csv_files = sort_by_middle_number(csv_files)\n",
    "#     csv_content = request.form.get('csv_edit_text') \n",
    "\n",
    "#     video_name = request.form.get('video_name')\n",
    "#     frame_id = request.form.get('frame_id')\n",
    "#     dataset_submission = seePrePost(video_name, int(frame_id))\n",
    "#     submission_samples.clear()\n",
    "#     for sample in dataset_submission:\n",
    "#         submission_samples.append(sample)\n",
    "#     session = fo.launch_app(dataset_submission, auto=False)\n",
    "\n",
    "#     processed_text_pre_and_post = f'Available at <a href=\"http://localhost:5151/datasets/pre_and_post\">http://localhost:5151/datasets/pre_and_post</a>'\n",
    "\n",
    "#     return render_template('index.html', notify_submit_text=user_text,\n",
    "#                     selected_csv_file=selected_csv_file, csv_files=csv_files,\n",
    "#                     csv_content=csv_content,\n",
    "#                     user_text = user_text, output_file=output_file, no_images=k, bias=bias,\n",
    "#                     discard_video=discard_video, processed_text_pre_and_post=processed_text_pre_and_post,\n",
    "#                     video_name=video_name, frame_id=frame_id)\n",
    "\n",
    "# app.run(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
